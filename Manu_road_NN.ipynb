{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Manu_road_NN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v33aCiDyTJNF",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYdofvDB8ur_",
        "colab_type": "text"
      },
      "source": [
        "- The following notebook corresponds to the experimentation and implementation of neural networks for the article <insert DOI here>. Data importing, pre-processing and sampling can be performed following the code presented in https://github.com/glarreag/Manu_road\n",
        "- This Jupyter notebooks contains snippets from code presented in [GEE tutorials](https://developers.google.com/earth-engine/tf_examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1dYzXmve8Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())\n",
        "print(tf.__version__)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WALPWfdDA2rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "fb2580b7-b79a-4417-9f12-9ebc74f364c1"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=dc692ffc8f7ce2e244f1ef2f08119d07f14d5d5aa0b6ab22c26c1a8f802bfcf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.2 GB  | Proc size: 1.1 GB\n",
            "GPU RAM Free: 15927MB | Used: 353MB | Util   2% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CsOaqllIU0bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U PyDrive\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import itertools\n",
        "# import tensorflow as tf\n",
        "\n",
        "import scipy.cluster.hierarchy as hac\n",
        "\n",
        "\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_columns', None)\n",
        "import warnings\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
        "# from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Import authentication libraries\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "%matplotlib inline\n",
        "%config IPCompleter.greedy=True\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set()\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvNrv730U36I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Authenticating google account to import and export from drive and GCP\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S8uRmuGTYyY",
        "colab_type": "text"
      },
      "source": [
        "## Data import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMURHx0lTgZQ",
        "colab_type": "text"
      },
      "source": [
        "Training data can be imported directly from a Google Drive folder.\n",
        "\n",
        "If the html is:\n",
        "https://drive.google.com/drive/folders/1TFe4wT4pTcTwVs0bJwtvikv0BmbTxuBa\n",
        "\n",
        "the file id is: \n",
        "1TFe4wT4pTcTwVs0bJwtvikv0BmbTxuBa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRckEUfMftat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_id= \"<your folder id>\" # https://drive.google.com/drive/folders/1TFe4wT4pTcTwVs0bJwtvikv0BmbTxuBa\n",
        "# folder_id='1uXkcRXuQ5fFG0S8mLW67ewj_jsqAb_Ff'\n",
        "file_list = drive.ListFile({'q': \"'{0}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "\n",
        "# Download all files to the VM\n",
        "for file1 in file_list:\n",
        "  file_id=file1['id']\n",
        "  file_title = file1['title']\n",
        "  if '.csv' in file_title: #import only csv\n",
        "    drive.CreateFile({'id': file_id}).GetContentFile(file_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX5ib4EBWrxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check that the files are in this VM\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQXOGBe_U0cG",
        "colab_type": "text"
      },
      "source": [
        "## Exploring variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUzMKO_YMFrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data sample from 2004 - 2017 period\n",
        "#WITHOUT LAN LON\n",
        "file_name = \"<your_file_name_here.csv>\"\n",
        "\n",
        "# discard unused variables, the following variables are always included by GEE exporting \n",
        "training = pd.read_csv(file_name, sep=',').drop(['system:index', '.geo','cluster_mean', \"random\"],axis=1)  \n",
        "validation = pd.read_csv(file_name, sep= \",\").drop(['system:index', '.geo','cluster_mean', \"random\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Jk4-dgDhd1",
        "colab_type": "text"
      },
      "source": [
        "Data for prediction was exported following the script [link to earth engine exporting scrpit] as TFRecord files. During task preparation it is important to specify the patch_size and the max_size parameteres. The complete image to predict is divided in patches of dimensions  [patch_size, patch_size]. The TFRecord file is divided in various TFRecord files of size=max_size.\n",
        "\n",
        "An aditional \"mixer\" json file is exported. This file is necessary to map the predicted file into the Earth Engine plataform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKkUA81seSHS",
        "colab_type": "text"
      },
      "source": [
        "## Model construction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xban-5ZlGe28",
        "colab_type": "text"
      },
      "source": [
        "Fundamental layers definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypFhmmX6G__u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch Normalization func\n",
        "def batch_norm(x, mode = None, train=True, name=\"batch_norm\", training=False): # using batchnormalization training (moving means) \n",
        "  with tf.variable_scope(name):\n",
        "    normie=tf.layers.batch_normalization(x, name=name,momentum=0.90, training=training)\n",
        "    return normie\n",
        "    \n",
        "#Linear activation\n",
        "def dense(input_, output_size, name=None, stddev=0.02, bias_start=0.0, regularization='none' ,activation='relu'):\n",
        "    shape = input_.get_shape().as_list()\n",
        "    \n",
        "    if regularization == 'l2': #If regularization is wanted. Only expects 'l2' regularization\n",
        "      with tf.variable_scope(name):\n",
        "          matrix = tf.get_variable(\"Weights\", [shape[1], output_size], tf.float32,\n",
        "                                   tf.initializers.glorot_normal(seed=0), regularizer=tf.keras.regularizers.l2(l=0.005))\n",
        "          bias = tf.get_variable(\"bias\", [output_size],\n",
        "              initializer=tf.constant_initializer(bias_start))\n",
        "          linear = tf.matmul(input_, matrix) + bias\n",
        "          if activation == 'relu':\n",
        "            return tf.nn.relu(linear, name='activation_relu')\n",
        "          if activation == 'sigmoid':\n",
        "            return tf.nn.sigmoid(linear, name='activation_relu')\n",
        "          if activation == 'logits':\n",
        "            return linear\n",
        "          \n",
        "    if regularization == 'none':\n",
        "      with tf.variable_scope(name):\n",
        "          matrix = tf.get_variable(\"Weights\", [shape[1], output_size], tf.float32,\n",
        "                                   tf.initializers.glorot_normal(seed=0))\n",
        "          bias = tf.get_variable(\"bias\", [output_size],\n",
        "              initializer=tf.constant_initializer(bias_start))\n",
        "          linear = tf.matmul(input_, matrix) + bias\n",
        "          if activation == 'relu':\n",
        "            return tf.nn.relu(linear, name='activation_relu')\n",
        "          if activation == 'sigmoid':\n",
        "            return tf.nn.sigmoid(linear, name='activation_relu')\n",
        "          if activation == 'logits':\n",
        "            return linear      \n",
        "        \n",
        "#Dropout      \n",
        "def dropout(input_,name='dropout', training=False, mode = None):\n",
        "  dropped = tf.layers.dropout(\n",
        "                inputs=input_,\n",
        "                rate=0.1,\n",
        "                noise_shape=None,\n",
        "                seed=None,\n",
        "                training = training,\n",
        "                name=name)\n",
        "  return dropped\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdl-D47oMT1N",
        "colab_type": "text"
      },
      "source": [
        "Different network architectures can be defined in this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcoT9mRBL3us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def net_0(x,training=False): \n",
        "  \n",
        "  d1= dense(x, 256,name='d1')\n",
        "  d2= dense(d1,128,name ='d2')\n",
        "  d3 = dense(d2,64, name = 'd3')\n",
        "  d4 = dense(d3,32, name = 'd4')\n",
        "  d5 = dense(d4,16, name = 'd5')\n",
        "  logits = dense(d5,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d5,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "def net_0_reg(x, mode='train',training=False): \n",
        "  \n",
        "  d1= dense(x, 256,name='d1',regularization='l2')\n",
        "  d2= dense(d1,128,name ='d2',regularization='l2')\n",
        "  d3 = dense(d2,64, name = 'd3',regularization='l2')\n",
        "  d4 = dense(d3,32, name = 'd4',regularization='l2')\n",
        "  d5 = dense(d4,16, name = 'd5',regularization='l2')\n",
        "  logits = dense(d5,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d5,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "def net_0_1(x, training=False):\n",
        "  \n",
        "  d1= dense(x, 1024,name='d1')\n",
        "  d2= dense(d1,1024,name ='d2')\n",
        "  d3 = dense(d2,512, name = 'd3')\n",
        "  d4 = dense(d3,256, name = 'd4')\n",
        "  d5 = dense(d4,128, name = 'd5')\n",
        "  d6 = dense(d5,56, name = 'd6') \n",
        "  dp0= dropout(d6,name='dp0')\n",
        "  logits = dense(dp0,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d6,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "\n",
        "\n",
        "def net_1_reg(x,training=False): \n",
        "  \n",
        "  d1= dense(x, 512,name='d1',regularization='l2')\n",
        "  bn1 = batch_norm(d1,name='bn1',training=training)\n",
        "  dp0= dropout(bn1,name='dp0',training=training)\n",
        "  d2= dense(dp0,128,name ='d2',regularization='l2')\n",
        "  bn2 = batch_norm(d2,name='bn2',training=training)\n",
        "  dp1= dropout(bn2,name='dp1',training=training)\n",
        "  d3 = dense(dp1,64, name = 'd3',regularization='l2')\n",
        "  bn3 = batch_norm(d3,name='bn3',training=training)\n",
        "  dp2= dropout(bn3,name='dp2',training=training)\n",
        "  d4 = dense(dp2,32, name = 'd4',regularization='l2')\n",
        "  bn4 = batch_norm(d4,name='bn4',training=training)\n",
        "  dp3= dropout(bn4,name='dp3',training=training)\n",
        "  d5 = dense(dp3,16, name = 'd5',regularization='l2')\n",
        "  bn5 = batch_norm(d5,name='bn5',training=training)\n",
        "  logits = dense(bn5,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(bn5,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "\n",
        "\n",
        "def nn_6_1024(x,mode='train',training=False):\n",
        "  \n",
        "  d1= dense(x, 1024,name='d1')\n",
        "  bn1 = batch_norm(d1,name='bn1',mode=mode)\n",
        "  d2= dense(bn1,512,name ='d2')\n",
        "  dp1= dropout(d2,name='dp1',mode=mode)\n",
        "  d3 = dense(dp1,256, name = 'd3')\n",
        "  bn3 = batch_norm(d3,name='bn3',mode=mode)\n",
        "  d4 = dense(bn3,128, name = 'd4')\n",
        "  dp3= dropout(d4,name='dp3',mode=mode)\n",
        "  d5 = dense(dp3,64, name = 'd5')\n",
        "  dp5= dropout(d5,name='dp5',mode=mode)\n",
        "  d6 = dense(dp5,32, name = 'd6')\n",
        "  logits = dense(d6,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d6,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "def nn_6_1024_reg(x,training=False):\n",
        "  \n",
        "  d1= dense(x, 1024,name='d1',regularization='l2')\n",
        "  bn1 = batch_norm(d1,name='bn1',training=training)\n",
        "  d2= dense(bn1,512,name ='d2',regularization='l2')\n",
        "  dp1= dropout(d2,name='dp1',training=training)\n",
        "  d3 = dense(dp1,256, name = 'd3',regularization='l2')\n",
        "  bn3 = batch_norm(d3,name='bn3',training=training)\n",
        "  d4 = dense(bn3,128, name = 'd4',regularization='l2')\n",
        "  dp3= dropout(d4,name='dp3',training=training)\n",
        "  d5 = dense(dp3,64, name = 'd5',regularization='l2')\n",
        "  dp5= dropout(d5,name='dp5',training=training)\n",
        "  d6 = dense(dp5,32, name = 'd6',regularization='l2')\n",
        "  logits = dense(d6,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d6,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "def nn_6_5000_reg(x,training=False): \n",
        "  \n",
        "  d1= dense(x, 2000,name='d1',regularization='l2')\n",
        "  bn1 = batch_norm(d1,name='bn1',training=training)\n",
        "  d2= dense(bn1,1000,name ='d2',regularization='l2')\n",
        "  dp1= dropout(d2,name='dp1',training=training)\n",
        "  d3 = dense(dp1,500, name = 'd3',regularization='l2')\n",
        "  d4 = dense(d3,600, name = 'd4',regularization='l2')\n",
        "  dp3= dropout(d4,name='dp3',training=training)\n",
        "  d5 = dense(dp3,100, name = 'd5',regularization='l2')\n",
        "  d6 = dense(d5,30, name = 'd6',regularization='l2')\n",
        "  logits = dense(d6,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d6,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "\n",
        "def nn_7_deep(x,training=False):\n",
        "  \n",
        "  d1= dense(x, 2048,name='d1',regularization='l2')\n",
        "  bn1 = batch_norm(d1,name='bn1',training=training)\n",
        "  d1_1= dense(bn1, 2048,name='d1_1',regularization='l2')\n",
        "  d2= dense(d1_1,1024,name ='d2',regularization='l2')\n",
        "  d3 = dense(d2,1024, name = 'd3',regularization='l2')\n",
        "  bn3 = batch_norm(d3,name='bn3',training=training)\n",
        "  d4 = dense(bn3,512, name = 'd4',regularization='l2')\n",
        "  d5 = dense(d4,264, name = 'd5',regularization='l2')\n",
        "  d6_1 = dense(d5,120, name = 'd6_1',regularization='l2')\n",
        "  d6 = dense(d6_1,32, name = 'd6',regularization='l2')\n",
        "  dp5= dropout(d6,name='dp5',training=training)\n",
        "  logits = dense(dp5,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(dp5,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "\n",
        "  return logits, predicted\n",
        "  \n",
        "def net_2(x,training=False): \n",
        "  \n",
        "  d1= dense(x, 60,name='d1')\n",
        "  d2= dense(d1,15,name ='d2')\n",
        "  logits = dense(d2,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d2,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n",
        "\n",
        "def reg_0(x):\n",
        "  \n",
        "  d1= dense(x, 120,name='d1')\n",
        "  bn1 = batch_norm(d1,name='bn1')\n",
        "  d2= dense(d1,60,name ='d2')\n",
        "  logits = dense(d2,1, name = 'logits', activation='logits')[:,0]\n",
        "  predicted = dense(d2,1, name = 'predicted', activation='sigmoid')[:,0]\n",
        "  return logits, predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZN-hCEAYAmI",
        "colab_type": "text"
      },
      "source": [
        "## Model definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7hwglNoZR7h",
        "colab_type": "text"
      },
      "source": [
        "The main layers are defined using the low level API of Tensorflow.\n",
        "Three main modes have been stablished:\n",
        "\n",
        "1.   **'train'**: Uses the csv training data imported as panda dataframe as input.\n",
        "2.   **'predict'**: Uses any panda dataframe data as input. If labels are included, the model returns the accuracy and the predicted labels. If no labels are included, the model only returns predicted labels\n",
        "3.   **'tfrecord'**: This mode is designed exclusively to make predictions from a TFRecord file, as exported from Earth Engine. The purpose of this mode is to predict large scale territories and returns a TFRecord file containing the predicted probabilities of each pixel, ready for Earth Engine ingestion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skvZvQefyYvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_prob = []\n",
        "predicted_int = []\n",
        "correct_bool = []\n",
        "\n",
        "def RUN(sess,X=None,y=None,X_val = None, y_val = None, X_test = None, y_test= None,PATCH_WIDTH=256, PATCH_HEIGHT=256,batch_size=256, epoch=100, learning_rate=0.001, \n",
        "        training=False,mode='train',name='model_1',test_file=None,mean_=None,scale_=None,new=False,model_folder=None, model_func=None):\n",
        "  \n",
        "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "  increment_global_step = tf.assign_add(global_step, 1, name = 'increment_global_step')\n",
        "  \n",
        "  if mode == 'train':\n",
        "    training=True\n",
        "  \n",
        "  if mode == 'tfrecord':  \n",
        "    X_tensor = predict_input_fn(test_file,mean_,scale_) #this ensure to have a tensor with values as X_tensor\n",
        "  else:\n",
        "    X_tensor= tf.placeholder(tf.float32, shape=(None,X.shape[1]),\n",
        "                                    name='X_tensor')\n",
        "  y_tensor= tf.placeholder(tf.float32, shape=(None),\n",
        "                                    name='y_tensor')\n",
        "    \n",
        "  def model(x,y=None,name='model_1', reg_loss=0, training=False):\n",
        "    with tf.variable_scope(name, reuse= tf.AUTO_REUSE):\n",
        "      logits, predicted = model_func(x,training=training)  \n",
        "    return logits, predicted \n",
        "\n",
        "  # with tf.variable_scope(name, reuse= tf.AUTO_REUSE):  \n",
        "  if mode =='train':\n",
        "    \n",
        "    logits, predicted = model(X_tensor, y_tensor,name=name, training=training)  \n",
        "\n",
        "    print(tf.global_variables())\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(_sentinel=None, \n",
        "            labels=y_tensor, logits=logits, name='loss'))\n",
        "    \n",
        "    reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    reg_loss = tf.reduce_sum(reg_loss)\n",
        "    tot_loss = tf.add(loss,reg_loss)\n",
        "    opt = tf.train.AdamOptimizer(learning_rate)\n",
        "    # opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    \n",
        "    \n",
        "    t_vars = tf.trainable_variables()\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "      model_opt=opt.minimize(tot_loss, var_list=t_vars)\n",
        "      \n",
        "    start_time = time.time()\n",
        "\n",
        "    print(tf.global_variables())\n",
        "    if new == True:\n",
        "      tf.global_variables_initializer().run()\n",
        "    else:\n",
        "      saver=tf.train.Saver()\n",
        "      ckpt = tf.train.get_checkpoint_state(model_folder_root+model_folder)\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      \n",
        "\n",
        "    print(tf.global_variables())\n",
        "    \n",
        "    for epoch in range(epoch):\n",
        "      X, y = shuffle(X,y)\n",
        "      batch_idxs = len(X) // batch_size\n",
        "      for idx in range(0, batch_idxs): \n",
        "        batch_X = X[idx*batch_size:(idx+1)*batch_size]\n",
        "        batch_y = y[idx*batch_size:(idx+1)*batch_size]\n",
        "        \n",
        "        sess.run(model_opt, feed_dict={X_tensor: batch_X, y_tensor: batch_y})\n",
        "      \n",
        "\n",
        "      loss_print=tf.reduce_mean(loss)\n",
        "      classified = tf.to_int32(predicted > 0.5)\n",
        "      correct_train = tf.equal(classified, y_train.values)\n",
        "      correct_val = tf.equal(classified, y_val.values)\n",
        "      correct_test = tf.equal(classified, y_test.values)\n",
        "      \n",
        "      train_loss = loss_print.eval({X_tensor: X_train, y_tensor: y_train})     \n",
        "      val_loss = loss_print.eval({X_tensor: X_val, y_tensor: y_val})\n",
        "      \n",
        "      train_acc = tf.reduce_mean(tf.cast(correct_train, tf.float32)).eval({X_tensor: X_train, y_tensor: y_train})\n",
        "      val_acc = tf.reduce_mean(tf.cast(correct_val, tf.float32)).eval({X_tensor: X_val, y_tensor: y_val})\n",
        "      test_acc = tf.reduce_mean(tf.cast(correct_test, tf.float32)).eval({X_tensor: X_test, y_tensor: y_test})\n",
        "      \n",
        "      sess.run(increment_global_step)\n",
        "      new_epoch = global_step.eval()\n",
        "      print(\"Epoch: [%2d] time: %4.4f, train_loss: %.8f, val_loss: %.8f, train_acc: %.8f, val_acc: %.8f, test_acc: %.8f\" \\\n",
        "                  % (new_epoch, time.time() - start_time, train_loss, val_loss,train_acc,val_acc,test_acc))\n",
        "    \n",
        "    saver=tf.train.Saver()\n",
        "    saver.save(sess, model_folder_root+model_folder+'model.ckpt', global_step=global_step)\n",
        "      \n",
        "\n",
        "  if mode =='predict':\n",
        "    \n",
        "    print('aquitoy_func')\n",
        "    print(tf.global_variables())\n",
        "    \n",
        "    logits, predicted = model(X_tensor, y_tensor,name='model_1',training=training)  \n",
        "\n",
        "    \n",
        "    saver=tf.train.Saver()\n",
        "    ckpt = tf.train.get_checkpoint_state(model_folder_root+model_folder)\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    print('aquitoy_predict')\n",
        "    print(tf.global_variables())\n",
        "    predicted_value=predicted.eval({X_tensor: X, y_tensor: y})\n",
        "  \n",
        "    print(predicted_value)\n",
        "    \n",
        "    predicted_prob.append(predicted_value)\n",
        "    classified = tf.to_int32(predicted > 0.5)\n",
        "    \n",
        "    classified_value = classified.eval({X_tensor: X, y_tensor: y})\n",
        "    predicted_int.append(classified_value)\n",
        "    \n",
        "    correct_prediction = tf.equal(classified, y)\n",
        "\n",
        "    correct_value = correct_prediction.eval({X_tensor: X, y_tensor: y})\n",
        "\n",
        "    correct_bool.append(correct_value)\n",
        "\n",
        "    print(correct_value)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).eval({X_tensor: X, y_tensor: y})\n",
        "    print('DA A-Q-RA-C', accuracy)\n",
        "\n",
        "  if mode =='tfrecord':\n",
        "    ##Makes an example_proto taking a predicted value as input\n",
        "    def make_example(pred):\n",
        "        return tf.train.Example(\n",
        "          features=tf.train.Features(\n",
        "            feature={\n",
        "              'probability': tf.train.Feature(\n",
        "                  float_list=tf.train.FloatList(\n",
        "                      value=pred))\n",
        "            }\n",
        "          )\n",
        "        )\n",
        "\n",
        "  \n",
        "    logits, predicted = model(X_tensor,name='model_1',training=training)\n",
        "    \n",
        "    \n",
        "    saver=tf.train.Saver()\n",
        "    ckpt = tf.train.get_checkpoint_state(model_folder_root+model_folder)\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    \n",
        "    patch = [[]]\n",
        "    curPatch = 1\n",
        "\n",
        "    writer = tf.python_io.TFRecordWriter(outputImageFile)\n",
        "    still_writing= True\n",
        "    start_time = time.time()     \n",
        "\n",
        "    while still_writing: #We predict only until reaching the patch size (batch size) so we can save it and export it as tfrecord again\n",
        "\n",
        "        \n",
        "      try:\n",
        "        value = predicted.eval()[0]\n",
        "        patch[0].append(value)\n",
        "\n",
        "\n",
        "        if (len(patch[0]) == PATCH_WIDTH * PATCH_HEIGHT):\n",
        "          print('Done with patch ' + str(curPatch) + '...'+' Elapsed time: {:.2f}'.format(time.time()-start_time))\n",
        "\n",
        "          # Create an example\n",
        "          example=make_example(patch[0])\n",
        "                \n",
        "          writer.write(example.SerializeToString())\n",
        "          patch = [[]]\n",
        "          curPatch += 1\n",
        "          start_time = time.time() \n",
        "\n",
        "      except: \n",
        "          still_writing=False\n",
        "    \n",
        "    writer.close()\n",
        "    print('bug')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKlFn7o4Zheo",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "1.   Introduce a folder to store checkpoint objects.\n",
        "2.   GCP buckets can be used too\n",
        "3.   Multiple models, number of epochs, and training samples can be evaluated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbFzyi7VbDiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "your_folder_root = \"<insert_your_folder>\" \n",
        "model_folder_root = your_folder_root\n",
        "epochs = [100, 300, 300, 700, 700]\n",
        "models = [net_0, net_0_reg,net_2,net_0_1,nn_6_5000_reg] #example\n",
        "datasets_names = [\"04_17\"] #example\n",
        "datasets_t = [training]\n",
        "datasets_v = [validation] \n",
        "model_folder =  ['net_0_noXY/', 'net_0_reg_noXY/','net_2_noXY/', 'net_0_1_noXY/','nn_6_5000_reg_noXY/'] #example\n",
        "model_names =  ['net_0', 'net_0_reg','net_2', 'net_0_1','nn_6_5000_reg'] #example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkGk4XIobNnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for training_set, validation_set, db_name in zip(datasets_t, datasets_v, datasets_names):\n",
        "  \n",
        "  X_trainval = training_set.drop('lossyear', axis=1)\n",
        "  y_trainval = training_set['lossyear']\n",
        "\n",
        "  X_test = validation_set.drop('lossyear', axis=1)\n",
        "  y_test = validation_set['lossyear']\n",
        "\n",
        "  scaler = preprocessing.RobustScaler().fit(X_trainval) #do not forget to use the same scaler object to predict\n",
        "\n",
        "  X_trainval = scaler.transform(X_trainval)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.3,random_state=0)\n",
        "\n",
        "  for model, folder, epoch, model_name in zip(models,model_folder, epochs, model_names):\n",
        "\n",
        "    folder = db_name + \"/\" + folder\n",
        "\n",
        "    g = tf.Graph()\n",
        "    with g.as_default():\n",
        "      with tf.Session() as sess:\n",
        "        RUN(X=X_train,y=y_train,X_val=X_val, y_val= y_val,X_test =X_test,y_test=y_test, sess=sess,epoch=epoch,batch_size = 250, learning_rate=0.0001, new=True, model_folder=folder, model_func=model)\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvBO95gLcOnj",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9miK8yVxcNMd",
        "colab": {}
      },
      "source": [
        "# repeating the normalization process to ensure the same output (use the same random_state)\n",
        "for training_set, validation_set, db_name in zip(datasets_t, datasets_v, datasets_names):\n",
        "  \n",
        "  X_trainval = training_set.drop('lossyear', axis=1)\n",
        "  y_trainval = training_set['lossyear']\n",
        "\n",
        "  X_test = validation_set.drop('lossyear', axis=1)\n",
        "  y_test = validation_set['lossyear']\n",
        "\n",
        "  scaler = preprocessing.RobustScaler().fit(X_trainval)\n",
        "\n",
        "  X_trainval = scaler.transform(X_trainval)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.3,random_state=0)\n",
        "\n",
        "  for model, folder, epoch, model_name in zip(models,model_folder, epochs, model_names):\n",
        "\n",
        "    folder = db_name + \"/\" + folder\n",
        "\n",
        "    predicted_prob = []\n",
        "    g = tf.Graph()\n",
        "    with g.as_default():\n",
        "      with tf.Session() as sess:\n",
        "        RUN(X=X_test,y=y_test.values,sess=sess,mode='predict', model_folder=folder,model_func=model)\n",
        "\n",
        "    for_dataframe= {\"cluster\":y_test, \"classification\":predicted_prob[0]}\n",
        "    df = pd.DataFrame(for_dataframe)\n",
        "    file_name = \"predicted_\"+db_name+\"_\"+model_name+\".csv\" \n",
        "    df.to_csv(file_name, sep=',', encoding='utf-8') # a csv file is created with the probability value \n",
        "\n",
        "\n",
        "    ## uncomment to export CSV to Google Drive folder\n",
        "    # file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n",
        "    # file.SetContentFile(file_name)\n",
        "    # file.Upload() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMN3fWOczTQ",
        "colab_type": "text"
      },
      "source": [
        "## Image prediction\n",
        "\n",
        "\n",
        "1.   The image to predict is in tfrecord format, exported from GEE (see data_ingestion.js)\n",
        "2.   A parsing function is defined. This function also normalize the data using the scaling parameters calculated during training.\n",
        "3. Tfrecord files are stored in a Google Cloud Storage bucket.\n",
        "4. The predicted imaged needs to be imported to GEE as a tfrecord. An authentication step is required.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sXx7WKNefeD",
        "colab_type": "text"
      },
      "source": [
        "Creating tfrecord parsing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO1AnMKZa-nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Names of the features.\n",
        "bands = [\n",
        "  \"altitude\",\n",
        "  #  \"lossyear\",\n",
        "   \"latitude\",\n",
        "  \"distance_second\",\n",
        "  \"slope\",\n",
        "  \"landform\",\n",
        "  \"distance_parks\",\n",
        "  # \"carbon_density\",\n",
        "  \"distance_first\",\n",
        "  # \"cluster_mean\",\n",
        "  \"distance_villages\",\n",
        "  \"distance_buffer\",\n",
        "  \"distance_third\",\n",
        "  \"distance_any\"\n",
        "  \"longitude\"\n",
        "]\n",
        "\n",
        "bands.sort() #to match the order of exported variables\n",
        "featureNames = list(bands)\n",
        "\n",
        "PATCH_WIDTH = 256\n",
        "PATCH_HEIGHT = 256\n",
        "PATCH_DIMENSIONS_FLAT = [PATCH_WIDTH * PATCH_HEIGHT, 1] #calculates the number of pixel in each patch\n",
        "\n",
        "# Feature columns\n",
        "columns = [\n",
        "  tf.FixedLenFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32) for k in featureNames #Creates a feature for each band \n",
        "                                                                                          \n",
        "]\n",
        "\n",
        "featuresDict = dict(zip(featureNames, columns)) \n",
        "\n",
        "#This function parses the data and normalizes it before predicting \n",
        "def predict_input_fn(fileNames,mean_,scale_): #Se ha entrenado normalizando los datos con los datos de entrenamiento\n",
        "                                              \n",
        "  \n",
        "  driveDataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')  # Note that you can make one dataset from many files by specifying a list.\n",
        "  \n",
        "  #Parsing function\n",
        "  def parse(example_proto): \n",
        "    parsed_features = tf.parse_single_example(example_proto, featuresDict) #Parses tfrecord and returns dict-like object\n",
        "    \n",
        "    #to normalize, we need to have tensors: \n",
        "    altitude=tf.cast(parsed_features['altitude'], tf.float32),\n",
        "    distance_any=tf.cast(parsed_features['distance_any'], tf.float32),\n",
        "    distance_buffer=tf.cast(parsed_features['distance_buffer'], tf.float32),\n",
        "    distance_second=tf.cast(parsed_features['distance_second'], tf.float32),\n",
        "    distance_first=tf.cast(parsed_features['distance_first'], tf.float32),\n",
        "    distance_parks=tf.cast(parsed_features['distance_parks'], tf.float32),\n",
        "    distance_third=tf.cast(parsed_features['distance_third'], tf.float32),\n",
        "    distance_villages=tf.cast(parsed_features['distance_villages'], tf.float32),\n",
        "    slope=tf.cast(parsed_features['slope'], tf.float32),\n",
        "    landform=tf.cast(parsed_features['landform'], tf.float32),\n",
        "    latitude=tf.cast(parsed_features['latitude'], tf.float32),\n",
        "    longitude=tf.cast(parsed_features['longitude'], tf.float32),\n",
        "    \n",
        "\n",
        "    #the order matters, be careful with the order of bands\n",
        "    data = tf.concat([altitude,distance_any,distance_buffer,distance_first,\n",
        "              distance_parks,distance_second,distance_third,distance_villages,landform,slope, latitude, longitude],axis=0)\n",
        "    data = tf.transpose(data, [1, 0, 2])\n",
        "    return data\n",
        "\n",
        "  def normalize(value,mean_,scale_): \n",
        "    #inputs: el tensor sin normalizar, el vector de media y el vector de desviaciÃ³n est.\n",
        "    \n",
        "    mean = tf.convert_to_tensor(mean_, dtype=tf.float32) \n",
        "    mean = tf.transpose(tf.reshape(mean, [12,1]),[1,0])\n",
        "\n",
        "    scale=tf.convert_to_tensor(scale_, dtype=tf.float32) \n",
        "    scale= tf.transpose(tf.reshape(scale, [12,1]),[1,0])\n",
        "\n",
        "    new = tf.divide(tf.subtract(value,mean),scale) # normalizing\n",
        "    return new\n",
        "  \n",
        "  parsedDataset = driveDataset.map(parse, num_parallel_calls=5)\n",
        "  parsedDataset = parsedDataset.flat_map(lambda features: tf.data.Dataset.from_tensor_slices(features))  \n",
        "  # parsedDataset = parsedDataset.prefetch(1)\n",
        "  # parsedDataset = parsedDataset.cache()\n",
        "  iterator = parsedDataset.make_one_shot_iterator().get_next()\n",
        "  transposed =tf.transpose(iterator, [1,0])\n",
        "  tensor = normalize(transposed,mean_,scale_)\n",
        "  return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tUY6drGXrAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Identify tfrecord file \n",
        "files = !gsutil ls gs://roads_and_stuff/\n",
        "test_file=list(filter(lambda x: '.gz' in x, files))\n",
        "test_file #several TFRecord files conform the complete image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27W1GXnZeznv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mixer_file = list(filter(lambda x: '.json' in x and hint in x, files))\n",
        "mixer_file[0]\n",
        "#Counting the number of patches contained in each TFRecord file\n",
        "options = tf.python_io.TFRecordOptions('GZIP')\n",
        "for i in test_file:\n",
        "  size = sum(1 for _ in tf.python_io.tf_record_iterator(i, options=options))\n",
        "  print(\"The number of patches in \" + i + ' is:' , size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXB92jz7gXQt",
        "colab_type": "text"
      },
      "source": [
        "Creates a predictied tfrecord image file using every model defined in previous cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEMM_LUzg59f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseName = '<your_folder>'\n",
        "model_folder_root = your_folder_root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS7_ljcEe-gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for training_set, validation_set, db_name in zip(datasets_t, datasets_v, datasets_names):\n",
        "  \n",
        "  X_trainval = training_set.drop('lossyear', axis=1)\n",
        "  y_trainval = training_set['lossyear']\n",
        "\n",
        "  X_test = validation_set.drop('lossyear', axis=1)\n",
        "  y_test = validation_set['lossyear']\n",
        "\n",
        "  scaler = preprocessing.RobustScaler().fit(X_trainval)\n",
        "\n",
        "  X_trainval = scaler.transform(X_trainval)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.3,random_state=0)\n",
        "\n",
        "  for model, folder, epoch, model_name in zip(models,model_folder, epochs, model_names):\n",
        "\n",
        "    folder = db_name + \"/\" + folder\n",
        "    outputImageFile = baseName + 'predictions_'+db_name+'_'+model_name+'.tfrecord'\n",
        "    outputJsonFile = baseName + 'test_predictions.json'\n",
        "\n",
        "    g = tf.Graph()\n",
        "    with g.as_default():\n",
        "      with tf.Session() as sess:\n",
        "        sess.run(RUN(sess=sess,mode='tfrecord',test_file=test_file,model_folder=folder,model_func = model ,mean_=scaler.center_,scale_=scaler.scale_))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFEJIpl4hjSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate\n",
        "!pip install earthengine-api\n",
        "!earthengine authenticate --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONUPt7mSid3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minAVcxxhDRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#insert your folder information\n",
        "gee_folder = \"users/<your_user_name>/<your_folder_name>/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oak0G5ILfYCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for training_set, validation_set, db_name in zip(datasets_t, datasets_v, datasets_names):\n",
        "  \n",
        "  X_trainval = training_set.drop('lossyear', axis=1)\n",
        "  y_trainval = training_set['lossyear']\n",
        "\n",
        "  X_test = validation_set.drop('lossyear', axis=1)\n",
        "  y_test = validation_set['lossyear']\n",
        "\n",
        "  scaler = preprocessing.RobustScaler().fit(X_trainval)\n",
        "\n",
        "  X_trainval = scaler.transform(X_trainval)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.3,random_state=0)\n",
        "\n",
        "  for model, folder, epoch, model_name in zip(models,model_folder, epochs, model_names):\n",
        "\n",
        "    folder = db_name + \"/\" + folder\n",
        "\n",
        "    outputImageFile = baseName + 'predictions_'+db_name+'_'+model_name+'.tfrecord'\n",
        "    outputJsonFile = baseName + 'test_predictions.json'\n",
        "\n",
        "    outputAssetID = gee_folder+f'{db_name}_{model_name}' \n",
        "\n",
        "    !earthengine upload image --asset_id={outputAssetID} {outputImageFile} {mixer_file[0]}\n",
        "    print(f\"done with {model_name} of {db_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}